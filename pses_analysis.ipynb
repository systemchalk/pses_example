{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of 2020 PSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a Jupyter notebook which allows a user to produce markdown between fragments of code. This will require a working instance of Python the pandas, numpy, and scipy modules (most likely through using the Anaconda distribution). Otherwise this document should be readable but the code will not run. Comments on the PSES will be written with brief explanations of the code.\n",
    "\n",
    "## Setup\n",
    "We begin by importing the necessary modules and the PSES file. The preferred file does not have labels but is easy to manipulate computationally. For some reason this file has an unusual encoding (\"ANSI\") and so we read it into Pandas to make it easy to manipulate and include an argument to ensure it is read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load Data Set\n",
    "pse = pd.read_csv(\"2020-public-service-employee-survey-open-dataset-ensemble-de-donnees-ouvertes-du-sondage-aupres-.csv\", encoding = \"ANSI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not interested in the entire PSES and so reduce it down to subsets. The variables LEVEL1ID  - LEVEL4ID include numerical values to identify the organizational levels. There are 5 natural points of comparison (each a subset of the other with the exception of the last):\n",
    "\n",
    "* The Public Service as a whole\n",
    "* Statistics Canada\n",
    "* Economic Statistics\n",
    "* Economy Wide Statistics\n",
    "* Consumer Prices Division\n",
    "\n",
    "The final subset is the Producer Prices Division (PPD) itself which is what we are interested in comparing to everything else. These subsets simplify working with these comparsions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSE File\n",
    "# LEVEL1ID = 8   - Statistics Canada\n",
    "# LEVEL2ID = 203 - Economic Statistics Field\n",
    "# LEVEL3ID = 306 - Economy-wide Statistics Branch\n",
    "# LEVEL4ID = 417 - Producer Prices Division\n",
    "\n",
    "public_service = pse[(pse.LEVEL1ID == 0) & (pse.LEVEL2ID == 0) & \n",
    "                     (pse.LEVEL3ID == 0) & (pse.LEVEL4ID == 0) & \n",
    "                     (pse.BYCOND == \" \") & (pse.SURVEYR == 2020)]\n",
    "statcan = pse[(pse.LEVEL1ID == 8) & (pse.LEVEL2ID == 0) &\n",
    "              (pse.LEVEL3ID == 0) & (pse.LEVEL4ID == 0) &\n",
    "              (pse.BYCOND == \" \") & (pse.SURVEYR == 2020)]\n",
    "econ_stat = pse[(pse.LEVEL1ID == 8) & (pse.LEVEL2ID == 203) &\n",
    "                (pse.LEVEL3ID == 0) & (pse.LEVEL4ID == 0)]\n",
    "econ_wide_stat = pse[(pse.LEVEL1ID == 8) & (pse.LEVEL2ID == 203) &\n",
    "                     (pse.LEVEL3ID == 306) & (pse.LEVEL4ID == 0)]\n",
    "ppd = pse[(pse.LEVEL1ID == 8) & (pse.LEVEL2ID == 203) &\n",
    "          (pse.LEVEL3ID == 306) & (pse.LEVEL4ID == 417)]\n",
    "cpd = pse[(pse.LEVEL1ID == 8) & (pse.LEVEL2ID == 203) &\n",
    "          (pse.LEVEL3ID == 306) & (pse.LEVEL4ID == 416)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final section of the setup we define functions to help with the analysis. Because this document is instructional there are more functions written here than we will actually use. Each function is documented but this is an example of breaking down a problem into smaller components and then reusing those components.\n",
    "\n",
    "The essence of these functions is this. The mean results for each of the questions are compared using a t-test (specifically Welch's t-test which does not require the samples to be equal or the population variance for each sample to be equal though still performs reasonably well if they are). The end result is a data frame with the mean for each question from each subset, a p-value and adjusted significance level.\n",
    "\n",
    "A few notes on the approaches:\n",
    "* The fastest program is to simply run the test on the statistics directly. These statistics can be obtained from the survey using the proportions and and answer counts. This is the one the program uses.\n",
    "* The alternative is to recreate the sample and run the t-test directly from stats. This is implemented but not used.\n",
    "* A final choice is a bootstrap method. This is purely for illustration as the problem we are dealing with falls well within either Student's t-test (when adjusted for different sample sizes) or Welch's t-test. The biggest disadvantage of this approach is that each question requires about a minute and a half to two minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions to work with.\n",
    "\n",
    "# These helper functions are used in the analysis\n",
    "\n",
    "def calc_mean_se(df, q, rm_ppd = False):\n",
    "    \"\"\" Calculates the mean and standard error for a given DataFrame and\n",
    "    question based on the proportion. Returns a tuple with mean, se, and obs.\n",
    "    \n",
    "    df -- The DataFrame for the sample\n",
    "    q  -- The question\"\"\"\n",
    "    \n",
    "    ans_count = int(df[df.QUESTION == q].ANSCOUNT)\n",
    "    \n",
    "    # Condition to deal with Yes/No answers. Note, one implication of this fix\n",
    "    # is that 'Agree' is mapped to positive which should be kept in mind when\n",
    "    # reading questions like Q53\n",
    "    if q in [\"Q28\", \"Q53\", \"Q85\", \"Q87\", \"Q88\", \"Q89\", \"Q90\"]:\n",
    "        no_pos = int(((df[df.QUESTION == q].AGREE / 100) * \n",
    "                      ans_count).round())\n",
    "        no_neg = ans_count - no_pos\n",
    "    elif q == \"Q54\":\n",
    "        print(\"Q54 is a special case, treat it manually.\")\n",
    "        return(0, 0, 1)\n",
    "    else:\n",
    "        no_pos = int(((df[df.QUESTION == q].MOST_POSITIVE_OR_LEAST_NEGATIVE / 100) * \n",
    "                      ans_count).round())\n",
    "        no_neg = int(((df[df.QUESTION == q].MOST_NEGATIVE_OR_LEAST_POSITIVE / 100) * \n",
    "                      ans_count).round())\n",
    "        \n",
    "    if rm_ppd:\n",
    "        prop_pos, _, ppd_n = calc_mean_se(ppd, q)\n",
    "        ppd_pos = round(prop_pos * ppd_n)\n",
    "        ppd_neg = ppd_n - ppd_pos\n",
    "        \n",
    "        no_pos -= ppd_pos\n",
    "        no_neg -= ppd_neg\n",
    "\n",
    "    total = no_pos + no_neg\n",
    "    \n",
    "    prop_pos = no_pos / total\n",
    "    prop_neg = no_neg / total\n",
    "    \n",
    "    # Calculate sum of squares\n",
    "    ss = (no_pos * (prop_neg ** 2)) + (no_neg * (prop_pos ** 2))\n",
    "    se = np.sqrt(ss / (total - 1))\n",
    "    \n",
    "    return (prop_pos, se, total)\n",
    "    \n",
    "def ttest_means_from_stats(df1, df2, q, equal = False, subset = True):\n",
    "    \"\"\" Applies a t-test to see if the means between two samples are\n",
    "    statistically different without generating samples. Returns a tuple\n",
    "    with two means and a p-value.\n",
    "    \n",
    "    df1   -- DataFrame for first sample\n",
    "    df2   -- DataFrame for second sample\n",
    "    q     -- Question to test samples for\n",
    "    equal -- Argument for t-test, default is to assume variance is not equal\n",
    "    \n",
    "    Note on equality: in our application equal should = True but this is the\n",
    "    safer default\"\"\"\n",
    "    \n",
    "    mean1, se1, nobs1 = calc_mean_se(df1, q)\n",
    "    mean2, se2, nobs2 = calc_mean_se(df2, q, rm_ppd = subset)\n",
    "    \n",
    "    if not(mean1 or mean2):\n",
    "        return (0, 0, 1)\n",
    "    \n",
    "    p_value = stats.ttest_ind_from_stats(mean1, se1, nobs1,\n",
    "                                         mean2, se2, nobs2, \n",
    "                                         equal_var = equal)[1]\n",
    "    \n",
    "    return (mean1 * 100, mean2 * 100, p_value)\n",
    "\n",
    "def make_comparisons(df1, df2, subset = True):\n",
    "    \"\"\" Helper function to produce a data frame of comparisons sorted by\n",
    "    p-values with a column for the Holm-Bonferonni correction.\n",
    "    \n",
    "    df1 -- Should be ppd, but left as a variable to subset on questions\n",
    "    df2 -- The data frame to compare against\"\"\"\n",
    "    \n",
    "    ppd_means = []\n",
    "    comp_means = []\n",
    "    p_vals = []\n",
    "\n",
    "    for question in df1.QUESTION:\n",
    "        ppd_m, comp_m, p_val = ttest_means_from_stats(df1, df2, question, subset = subset)\n",
    "        ppd_means.append(ppd_m)\n",
    "        comp_means.append(comp_m)\n",
    "        p_vals.append(p_val)\n",
    "\n",
    "    ppd_df_dict = {\"question\": df1.QUESTION,\n",
    "                   \"ppd_mean\": ppd_means,\n",
    "                   \"comp_mean\": comp_means,\n",
    "                   \"p_val\": p_vals}\n",
    "\n",
    "    sorted_df = pd.DataFrame(ppd_df_dict).sort_values(\"p_val\")\n",
    "    array_count = np.flip(np.array(range(1, sorted_df.shape[0] + 1)))\n",
    "    sorted_df[\"adj_p_val\"] = 0.05 / array_count\n",
    "    sorted_df[\"HB_corr\"] = sorted_df[\"p_val\"] < sorted_df[\"adj_p_val\"]\n",
    "    \n",
    "    return sorted_df\n",
    "\n",
    "# These helper functions are for illustration\n",
    "\n",
    "def make_sample(df, q, rm_ppd = False):\n",
    "    \"\"\" Takes the positive/negative proportions from a given data frame and \n",
    "    turns them into an array.\n",
    "    \n",
    "    df -- DataFrame to take proportions from\n",
    "    q  -- Question to take proportions from\"\"\"\n",
    "    \n",
    "    ans_count = int(df[df.QUESTION == q].ANSCOUNT)\n",
    "    \n",
    "    # Condition to deal with Yes/No answers. Note, one implication of this fix\n",
    "    # is that 'Agree' is mapped to positive which should be kept in mind when\n",
    "    # reading questions like Q53\n",
    "    if q in [\"Q28\", \"Q53\", \"Q85\", \"Q87\", \"Q88\", \"Q89\", \"Q90\"]:\n",
    "        no_pos = int(((df[df.QUESTION == q].AGREE / 100) * \n",
    "                      ans_count).round())\n",
    "        no_neg = ans_count - no_pos\n",
    "    elif q == \"Q54\":\n",
    "        print(\"Q54 is a special case, treat it manually.\")\n",
    "        return np.zeros(ans_count)\n",
    "    else:\n",
    "        no_pos = int(((df[df.QUESTION == q].MOST_POSITIVE_OR_LEAST_NEGATIVE / 100) * \n",
    "                      ans_count).round())\n",
    "        no_neg = int(((df[df.QUESTION == q].MOST_NEGATIVE_OR_LEAST_POSITIVE / 100) * \n",
    "                      ans_count).round())\n",
    "        \n",
    "    if rm_ppd:\n",
    "        prop_pos, _, ppd_n = calc_mean_se(ppd, q)\n",
    "        ppd_pos = round(prop_pos * ppd_n)\n",
    "        ppd_neg = ppd_n - ppd_pos\n",
    "        \n",
    "        no_pos -= ppd_pos\n",
    "        no_neg -= ppd_neg\n",
    "        ans_count -= ppd_n\n",
    "    \n",
    "    sample = np.zeros(ans_count)\n",
    "    sample[:no_pos] = 1\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def ttest_means(df1, df2, q, equal = False, subset = True):\n",
    "    \"\"\" Generates and applies a t-test to see if the means between two samples\n",
    "    are statistically different. Returns a tuple with two means and p-value.\n",
    "    \n",
    "    df1   -- DataFrame for first sample\n",
    "    df2   -- DataFrame for second sample\n",
    "    q     -- Question to test samples for\n",
    "    equal -- Argument for t-test, default is to assume variance is not equal\n",
    "    \n",
    "    Note on equality: in our application equal should = True but this is the\n",
    "    safer default\"\"\"\n",
    "    \n",
    "    sample1 = make_sample(df1, q)\n",
    "    sample2 = make_sample(df2, q, rm_ppd = subset)\n",
    "    \n",
    "    if not(sum(sample1) or sum(sample2)):\n",
    "        return (0, 0, 1)\n",
    "    \n",
    "    result = stats.ttest_ind(sample1, sample2, equal_var = equal)\n",
    "    \n",
    "    mean1 = sample1.mean() * 100\n",
    "    mean2 = sample2.mean() * 100\n",
    "    \n",
    "    return (mean1, mean2, result[1])\n",
    "\n",
    "# Note: Permutation test should have a seed set at the start of the file and\n",
    "# may run slowly with default values\n",
    "\n",
    "def perm_test(df1, df2, q, boot = 30000, subset = True):\n",
    "    \"\"\" Runs a permutation test to calculate difference between means when\n",
    "    variance is unknown. Returns a tuple with two means and a p-value.\n",
    "    \n",
    "    df1  -- DataFrame for first sample\n",
    "    df2  -- DataFrame for second sample\n",
    "    q    -- Question to test samples for\n",
    "    boot -- Number of runs to perform. Default is 30,000\"\"\"\n",
    "    \n",
    "    sample1 = make_sample(df1, q)\n",
    "    sample2 = make_sample(df2, q, rm_ppd = subset)\n",
    "    abs_mean_diff = abs(sample1.mean() - sample2.mean())\n",
    "    all_data = np.hstack([sample1, sample2])\n",
    "    extreme_vals = 0 # Initialize extreme values\n",
    "    \n",
    "    for _ in range(boot):\n",
    "        np.random.shuffle(all_data)\n",
    "        # Create two random samples drawn from all data\n",
    "        sample_a = all_data[:sample1.size]\n",
    "        sample_b = all_data[sample1.size:]\n",
    "        if abs(sample_a.mean() - sample_b.mean()) >= abs_mean_diff:\n",
    "            extreme_vals += 1\n",
    "            \n",
    "    # p-value is the proportion of extreme observations\n",
    "    p_value = extreme_vals / boot\n",
    "        \n",
    "    mean1 = sample1.mean() * 100\n",
    "    mean2 = sample2.mean() * 100\n",
    "    \n",
    "    return (mean1, mean2, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The main question we've been asked is how we can improve. A natural starting place is to see where PPD rates lower than its comparison. The next five expressions run the comparisons from the functions above. We can compare the means to see which questions PPD rates lower on, but the result would leave us with over 70 questions to examine. Most of these are likely minor and so we may be more interested in seeing statistically different results. We'll start by comparing the questions for which PPD rates lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n",
      "Q54 is a special case, treat it manually.\n"
     ]
    }
   ],
   "source": [
    "ppd_statcan = make_comparisons(ppd, statcan)\n",
    "ppd_econ_stat = make_comparisons(ppd, econ_stat)\n",
    "ppd_econ_wide_stat = make_comparisons(ppd, econ_wide_stat)\n",
    "ppd_ps = make_comparisons(ppd, public_service)\n",
    "ppd_cpd = make_comparisons(ppd[ppd.QUESTION.isin(set(cpd.QUESTION))], cpd, subset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "statcan_lower = set(ppd_statcan.loc[ppd_statcan.ppd_mean < \n",
    "                                    ppd_statcan.comp_mean, \"question\"])\n",
    "econ_stat_lower = set(ppd_econ_stat.loc[ppd_econ_stat.ppd_mean < \n",
    "                                    ppd_econ_stat.comp_mean, \"question\"])\n",
    "econ_wide_stat_lower = set(ppd_econ_wide_stat.loc\n",
    "                           [ppd_econ_wide_stat.ppd_mean <\n",
    "                            ppd_econ_wide_stat.comp_mean, \"question\"])\n",
    "ps_lower = set(ppd_ps.loc[ppd_ps.ppd_mean < \n",
    "                          ppd_ps.comp_mean, \"question\"])\n",
    "cpd_lower = set(ppd_cpd.loc[ppd_cpd.ppd_mean <\n",
    "                            ppd_cpd.comp_mean, \"question\"])\n",
    "\n",
    "ppd_statcan = make_comparisons(ppd[ppd.QUESTION.isin(statcan_lower)], statcan)\n",
    "ppd_econ_stat = make_comparisons(ppd[ppd.QUESTION.isin(econ_stat_lower)],\n",
    "                                 econ_stat)\n",
    "ppd_econ_wide_stat = make_comparisons(\n",
    "    ppd[ppd.QUESTION.isin(econ_wide_stat_lower)], econ_wide_stat)\n",
    "ppd_ps = make_comparisons(ppd[ppd.QUESTION.isin(ps_lower)], public_service)\n",
    "ppd_cpd = make_comparisons(ppd[ppd.QUESTION.isin(set(cpd_lower))], cpd, subset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q18h', 'Q70p', 'Q70x'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_statcan.loc[ppd_statcan[\"p_val\"] < 0.05, \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q18h', 'Q28', 'Q70b', 'Q70p', 'Q70w', 'Q70x', 'Q93'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_econ_stat.loc[ppd_econ_stat[\"p_val\"] < 0.05, \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q28', 'Q70b', 'Q70x', 'Q93'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_econ_wide_stat.loc[ppd_econ_wide_stat[\"p_val\"] < 0.05, \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q18h', 'Q70p', 'Q70x'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_ps.loc[ppd_ps[\"p_val\"] < 0.05, \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q70b', 'Q70c', 'Q70w', 'Q93'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_cpd.loc[ppd_cpd[\"p_val\"] < 0.05, \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are above but we will summarize them here since the code may not be immediately intuitive:\n",
    "\n",
    "At a 5% significance level PPD lags behind each comparison in the following questions:\n",
    "* Public Service: Q18h, Q70p, Q70x\n",
    "* Statistics Canada: Q18h, Q70p, Q70x\n",
    "* Economic Statistics: Q18h, Q28, Q70b, Q70p, Q70w, Q70x, Q93\n",
    "* Economy Wide Statistics: Q28, Q70b, Q70x, Q93\n",
    "* Consumer Prices : Q70b, Q70c, Q70w, Q93\n",
    "\n",
    "Some common patterns have emerged among the categories:\n",
    "\n",
    "* Q18h - \"My work suffers because of unreliable technology\"\n",
    "* Q70b - \"Overall to waht extent do the following factors cause you stress at work? Pay or other compensation-related issues\n",
    "* Q70p - \"Overall to what extent do the following factors cause you stress at work? Difficulty accessing my work tools or network\"\n",
    "* Q70x - \"Overall to what extend to the following factors cause you stress at work? Personal issues\"\n",
    "\n",
    "Broadly speaking, compared to the field level and above PPD appears to lag in terms of access to technology and the stress resulting from it. At lower levels (branch and comparable division) comparisons change in terms of factors contributing to stress with a focus on compensation, including Phoenix related issues.\n",
    "\n",
    "The difficulty here is that we are testing potentially 216 different questions for differences between means. At a 5% significance level it is entirely possible (even likely) there's at least one Type I error. We may feel more secure in the consistency between the results (technology is cited as a difficulty and also emerges as a source of stress). We do not need to rely on judgement to make this determination but can apply a correction. The make_comparisons function has already applied the Holm-Bonferroni correction (applying a stricter significance level based on the number of comparisons being made).\n",
    "\n",
    "The results are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_statcan.loc[ppd_statcan[\"HB_corr\"], \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_econ_stat.loc[ppd_econ_stat[\"HB_corr\"], \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_econ_wide_stat.loc[ppd_econ_wide_stat[\"HB_corr\"], \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q18h'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_ps.loc[ppd_ps[\"HB_corr\"], \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ppd_cpd.loc[ppd_cpd[\"HB_corr\"], \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only significant difference after the Holm-Bonferroni correction is a comparison between PPD and the public service in terms of unreliable technology. It is worth noting that the Public Service subset has the smallest number of questions (37) to test the hypothesis against and so these results are almost certainly driven by the number of questions being tested. As such, we do not need to abandon the conclusions that come from the hypothesis tests above but these conclusions are not immune to concerns about being the result of mining.\n",
    "\n",
    "The distinction is worth making when considering the comparisons that are being made. The Holm-Bonferroni correction is, ultimately, an adjustement to the significance level we tolerate and that significance level is itself a judgement call. As mentioned above, if the concern is that the significant results are the products of random chance, we can feel safer in the fact that there is a logical consistency between the technology questions.\n",
    "\n",
    "Another potential set of internally consistent results are the compensation related questions. Q93 is the question related to Phoenix (against which PPD falls behind Economic Statistics and Consumer Prices at a statistically significant level, but not against Statistics Canada or the Public Service at large) and Q70b list pay and compensation related issues as a source of stress. Unfortunately the remaining remaining questions do not lend themselves to this kind of internal validation. Q70w (lack of job security with unfavourable comparisons against Economic Statistics and Consumer Prices (CPD)), Q70b (pay or other compensation related issues, unfavourable comparison to CPD), and Q70c (heavy workload, unfavourable comparison against CPD) are all relevant but may or may not touch on related issues (e.g. overworked employees may feel underpaid, but overwork may also stem from access to technology).\n",
    "\n",
    "Why compare with CPD? While the two divisions are separate for a reason, it offers the most natural comparison in terms of the type of work that will be done. For instance, when comparing to Economic Statistics the nature of the work in the other divisions may mean that concerns about technology reflect PPD's different technological needs and the fact that they are difficult to address. If this difference persists between PPD and CPD then it invites the question as to why two divisions doing similar work face such different technological outcomes and recommends a different solution (either learning from what CPD has done to mitigate technological issues, or identify the pain points at PPD that are leading us to suffer more technological difficulties).\n",
    "\n",
    "An alternative perspective is to see the other comparisons as increasingly granular looks at PPD and its organization parents (which lends itself to the interpretation that differences are due to factors affecting the organizational unit. e.g. PPD is affected by technological outages more than its peers at StatCan), while the CPD comparison looks at similar work (which lends itself to an interpretation as to why there are better or worse outcomes for similar work. e.g. Why does PPD feel overworked compared to CPD?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "These results are an extract from a larger analysis of the PSES but were the most likely to enhance the work done by the existing team. The results can be summarized by organizational level.\n",
    "\n",
    "### Comparisons at the Field Level and Higher\n",
    "Compared to Statistics Canada and the Public Service as a whole, PPD has a statistically significant gap in questions related to technology, specifically questions Q18h and Q70p (work suffering because of unreliable technology and difficulty accessing work tools being a source of stress). This gap ranged from 27 to 18% fewer positive responses (though the work stress technology question was not significant for the field). In addition, PPD showed a smaller but still statistically significant gap in responses related to personal issues being a source of stress at work (Q70x). While we cannot rule out the possibility that this is a statistical aberration, it remains the most natural place outside of technology to examine due to its large magnitude.\n",
    "\n",
    "Compared to the Economic Statistics Field PPD also expressed greater concern regarding its job security. While it should be noted that the field as a whole is quite high (95% positive compared to PPD's 83%), a 12 point gap regarding job security is notable and is a clear issue that can be examined and addressed (as opposed to the more intangible personal issues).\n",
    "\n",
    "### Comparisons at the Branch Level\n",
    "Once checking below the Field level the technology gap gives way to other concerns. Pay and compensation related issues and personal issues were major sources of stress. These may be related to the Phoneix issues (Q93), but there is wider room for interpretation between a combination of Q70b (pay related stress) and Q93 (Phoenix issues), than there is the technology questions (Q18h and Q70p).\n",
    "\n",
    "### Comparisons to CPD\n",
    "PPD was compared to CPD given the potential similarity to the nature of the work. When comparing to CPD the significant differences regarding the technology questions (Q18h and Q70p) were not present (PPD's mean was lower, but by a small magnitude). Notably the significant differences related to job security (Q70w) and Phoenix (Q93) persist. While Phoneix issues are not likely to be related to the nature of the work, the difference in stresses related to job security between the field and a comparable division invites further scrutiny. In addition, heavy workload as a source of stress emerges as a factor against which PPD does not favourably compare.\n",
    "\n",
    "### Conclusions\n",
    "The absence of technological differences between the two prices divisions may suggest that these divisions have more demanding technological needs that are not being met by the existing divisions (or, alternatively, are not organized in a way that is harmonious with the delivery of technological services). Regardless of the source, technology affecting the quality of the work and becoming a source of stress is one of the most consistent results from the PSES.\n",
    "\n",
    "Beyond technological issues issues, PPD appears to be more heavily affected by Phoneix pay issues and concerns about job security at the branch (and lower) level. While we do not find evidence that PPD is any different at the field level or above, the persistence of the gap when comparing against the branch and anotehr division recommends further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
